{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import discourse.data as dd\n",
      "import discourse.inference.svm as disvm\n",
      "from discourse.models import barzilay as dmb\n",
      "from discourse.models import entity_grid as eg\n",
      "from discourse.inference import lp as dlp\n",
      "\n",
      "import numpy as np\n",
      "from os import remove, getenv\n",
      "from os.path import join\n",
      "\n",
      "from tempfile import NamedTemporaryFile\n",
      "\n",
      "ddir = getenv(\"DISCOURSEDIR\",\".\")\n",
      "eval_script = join(ddir, \"eval\", \"ordering-eval.py\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Model 0: Rank SVM Trained Entity Grid Model using CoreNLP for Dependencies/Coreference Resolution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<b>Model 0</b> is essentially the entity grid model from Barzilay & Lapata (2005) updated using a modern NLP stack to construct the entity grid. As in that paper, coreference is performed on the original ordering first. We then generate 20 random permutations and use these to learn feature weights using a ranking SVM."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NTSB Corpus"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training (SVM Rank)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Rank SVM trainer to generate 20 permutations \n",
      "# for each coherent instance.\n",
      "trainer = disvm.RankSVMTrainer(20)\n",
      "\n",
      "# Read in xml output of CoreNLP and create \n",
      "# training instances.\n",
      "for train_xml_file in dd.corenlp_ntsb_train():\n",
      "    model = dmb.new_barzilay_model(open(train_xml_file, 'r'),\n",
      "                                   max_salience=2,\n",
      "                                   history=2)\n",
      "    \n",
      "    trainer.add_model(model)\n",
      "\n",
      "trainer.train()\n",
      "\n",
      "# Output of training is a weight vector which\n",
      "# we will use to evaluate our test set.\n",
      "ntsb_weights = trainer.weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Temporary locations to write gold/predicted\n",
      "# sentence orderings.\n",
      "pred_file = NamedTemporaryFile(delete=False)\n",
      "gold_file = NamedTemporaryFile(delete=False)\n",
      "\n",
      "# Number of correct under Barzilay & Lapata eval.\n",
      "pair_correct = 0\n",
      "# Total number of original/permutation pairs.\n",
      "total_pairs = 0\n",
      "# Total document instances\n",
      "n = 0\n",
      "\n",
      "avg_ktau = 0\n",
      "\n",
      "for test_xml_file in dd.corenlp_ntsb_test():\n",
      "    n += 1\n",
      "    model  = dmb.new_barzilay_model(open(test_xml_file, 'r'),\n",
      "                                    max_salience=2,\n",
      "                                    history=3)\n",
      "    \n",
      "    perms = disvm.permute_model(model, 20)\n",
      "\n",
      "    \n",
      "    maxModel = model\n",
      "    coherent_score = np.dot(ntsb_weights, model.get_trans_prob_vctr()) \n",
      "    maxScore = coherent_score \n",
      "\n",
      "    \n",
      "    for perm in perms:\n",
      "        total_pairs += 1\n",
      "        perm_score = np.dot(ntsb_weights, perm.get_trans_prob_vctr())\n",
      "        # Get result of pairwise SVM Rank evaluation.\n",
      "        if coherent_score > perm_score:\n",
      "            pair_correct += 1\n",
      "        \n",
      "        if perm_score > maxScore:\n",
      "            maxScore = perm_score\n",
      "            maxModel = perm\n",
      "    \n",
      "    \n",
      "    pred_file.write(maxModel.pretty_string().encode('utf-8'))\n",
      "    pred_file.write('\\n\\n')\n",
      "    pred_file.flush()\n",
      "    \n",
      "    gold_file.write(model.pretty_string().encode('utf-8'))        \n",
      "    gold_file.write('\\n\\n')\n",
      "    gold_file.flush()\n",
      "        \n",
      "    \n",
      "pred_file.close()\n",
      "pred_filename = pred_file.name\n",
      "gold_file.close()\n",
      "gold_filename = gold_file.name\n",
      "\n",
      "!python $eval_script --gold $gold_filename --predicted $pred_filename      \n",
      "print \"B&L2005 Acc.: {}\".format(pair_correct/float(total_pairs))\n",
      "\n",
      "remove(pred_file.name)  \n",
      "remove(gold_file.name)      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total documents: 100\r\n",
        "Total correct: 25\r\n",
        "Accuracy: 0.25\r\n",
        "Avg. Kendall's Tau 0.27160994561\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "B&L2005 Acc.: 0.805\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "APWS Corpus"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training (SVM Rank)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set Rank SVM trainer to generate 20 permutations \n",
      "# for each coherent instance.\n",
      "trainer = disvm.RankSVMTrainer(20)\n",
      "\n",
      "# Read in xml output of CoreNLP and create \n",
      "# training instances.\n",
      "for train_xml_file in dd.corenlp_apws_train():\n",
      "    model = dmb.new_barzilay_model(open(train_xml_file, 'r'),\n",
      "                                   max_salience=3,\n",
      "                                   history=2)\n",
      "    trainer.add_model(model)\n",
      "\n",
      "trainer.train()\n",
      "\n",
      "# Output of training is a weight vector which\n",
      "# we will use to evaluate our test set.\n",
      "apws_weights = trainer.weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Temporary locations to write gold/predicted\n",
      "# sentence orderings.\n",
      "pred_file = NamedTemporaryFile(delete=False)\n",
      "gold_file = NamedTemporaryFile(delete=False)\n",
      "\n",
      "# Number of correct under Barzilay & Lapata eval.\n",
      "pair_correct = 0\n",
      "# Total number of original/permutation pairs.\n",
      "total_pairs = 0\n",
      "# Total document instances\n",
      "n = 0\n",
      "\n",
      "for test_xml_file in dd.corenlp_apws_test():\n",
      "    n += 1\n",
      "    model  = dmb.new_barzilay_model(open(test_xml_file, 'r'),\n",
      "                                    max_salience=3,\n",
      "                                    history=2)\n",
      "    perms = disvm.permute_model(model, 20)\n",
      "\n",
      "    \n",
      "    maxModel = model\n",
      "    coherent_score = np.dot(apws_weights, model.get_trans_prob_vctr()) \n",
      "    maxScore = coherent_score \n",
      "\n",
      "    \n",
      "    for perm in perms:\n",
      "        total_pairs += 1\n",
      "        perm_score = np.dot(apws_weights, perm.get_trans_prob_vctr())\n",
      "        # Get result of pairwise SVM Rank evaluation.\n",
      "        if coherent_score > perm_score:\n",
      "            pair_correct += 1\n",
      "        \n",
      "        if perm_score > maxScore:\n",
      "            maxScore = perm_score\n",
      "            maxModel = perm\n",
      "    \n",
      "    \n",
      "    pred_file.write(maxModel.pretty_string().encode('utf-8'))\n",
      "    pred_file.write('\\n\\n')\n",
      "    pred_file.flush()\n",
      "    \n",
      "    gold_file.write(model.pretty_string().encode('utf-8'))        \n",
      "    gold_file.write('\\n\\n')\n",
      "    gold_file.flush()\n",
      "        \n",
      "    \n",
      "pred_file.close()\n",
      "pred_filename = pred_file.name\n",
      "gold_file.close()\n",
      "gold_filename = gold_file.name\n",
      "\n",
      "!python $eval_script --gold $gold_filename --predicted $pred_filename       \n",
      "print \"B&L2005 Acc.: {}\".format(pair_correct/float(total_pairs))\n",
      "\n",
      "remove(pred_file.name)  \n",
      "remove(gold_file.name)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total documents: 99\r\n",
        "Total correct: 27\r\n",
        "Accuracy: 0.272727272727\r\n",
        "Avg. Kendall's Tau 0.277318593271\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "B&L2005 Acc.: 0.775757575758\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}