{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import discourse.models.debug as ddebug\n",
      "import discourse.data as data\n",
      "import discourse.inference.perceptron as perceptron\n",
      "from  discourse.models.rush import RushModel\n",
      "import corenlp_xml as cnlp\n",
      "from collections import defaultdict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_apws = [cnlp.Document(xml) for xml in data.corenlp_apws_train()]\n",
      "train_ntsb = [cnlp.Document(xml) for xml in data.corenlp_ntsb_train()]\n",
      "test_apws = [cnlp.Document(xml) for xml in data.corenlp_apws_test()]\n",
      "test_ntsb = [cnlp.Document(xml) for xml in data.corenlp_ntsb_test()]\n",
      "ptron = perceptron.PerceptronTrainer(50)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = defaultdict(int)\n",
      "\n",
      "for doc in train_apws:\n",
      "    model = RushModel(doc, history=2, features=set(['role_match']))\n",
      "    gold_trans = model.gold_transitions()\n",
      "    for trans in gold_trans:\n",
      "        for f in model.feature_map(trans):\n",
      "            counts[f] += 1\n",
      "\n",
      "fcounts = [(f, counts[f]) for f in counts]\n",
      "fcounts.sort(reverse=True, key=lambda a:a[1])\n",
      "for c in fcounts:\n",
      "    print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('role1:nn:role0:nn', 152)\n",
        "('role1:nsubj:role0:nsubj', 132)\n",
        "('role1:dobj:role0:dobj', 99)\n",
        "('role1:dep:role0:dep', 72)\n",
        "('role1:nsubj:role0:dep', 55)\n",
        "('role1:appos:role0:appos', 52)\n",
        "('role1:prep_with:role0:prep_with', 43)\n",
        "('role1:dobj:role0:nn', 28)\n",
        "('role1:prep_of:role0:prep_of', 26)\n",
        "('role1:prep_in:role0:nn', 26)\n",
        "('role1:nsubj:role0:prep_with', 24)\n",
        "('role1:prep_of:role0:nn', 23)\n",
        "('role1:nsubjpass:role0:dep', 22)\n",
        "('role1:prep_in:role0:prep_in', 21)\n",
        "('role1:root:role0:root', 21)\n",
        "('role1:conj_or:role0:conj_or', 19)\n",
        "('role1:poss:role0:nn', 18)\n",
        "('role1:nn:role0:dobj', 18)\n",
        "('role1:tmod:role0:tmod', 18)\n",
        "('role1:nsubj:role0:root', 17)\n",
        "('role1:nsubjpass:role0:nsubjpass', 17)\n",
        "('role1:num:role0:nn', 17)\n",
        "('role1:root:role0:nn', 16)\n",
        "('role1:nsubjpass:role0:nsubj', 16)\n",
        "('role1:dobj:role0:dep', 15)\n",
        "('role1:appos:role0:dobj', 14)\n",
        "('role1:agent:role0:nsubj', 14)\n",
        "('role1:nsubj:role0:nn', 14)\n",
        "('role1:prep_on:role0:prep_on', 13)\n",
        "('role1:prep_of:role0:dobj', 13)\n",
        "('role1:prep_of:role0:dep', 13)\n",
        "('role1:conj_and:role0:conj_and', 12)\n",
        "('role1:appos:role0:nn', 12)\n",
        "('role1:dobj:role0:nsubj', 12)\n",
        "('role1:nsubjpass:role0:conj_or', 12)\n",
        "('role1:prep_of:role0:nsubj', 11)\n",
        "('role1:conj_or:role0:nsubjpass', 11)\n",
        "('role1:nn:role0:appos', 10)\n",
        "('role1:dobj:role0:appos', 9)\n",
        "('role1:prep_of:role0:appos', 9)\n",
        "('role1:dep:role0:nn', 9)\n",
        "('role1:poss:role0:dobj', 8)\n",
        "('role1:conj_or:role0:dobj', 8)\n",
        "('role1:pobj:role0:pobj', 8)\n",
        "('role1:conj_and:role0:dobj', 8)\n",
        "('role1:nsubj:role0:prep_of', 7)\n",
        "('role1:dobj:role0:conj_or', 6)\n",
        "('role1:conj_and:role0:nn', 6)\n",
        "('role1:prep_across:role0:nn', 5)\n",
        "('role1:prep_in:role0:dobj', 5)\n",
        "('role1:poss:role0:root', 5)\n",
        "('role1:dep:role0:nsubj', 5)\n",
        "('role1:dobj:role0:prep_with', 5)\n",
        "('role1:nsubjpass:role0:prep_with', 5)\n",
        "('role1:ccomp:role0:prep_on', 5)\n",
        "('role1:agent:role0:dep', 5)\n",
        "('role1:nsubj:role0:appos', 5)\n",
        "('role1:nn:role0:dep', 4)\n",
        "('role1:prep_with:role0:appos', 4)\n",
        "('role1:dobj:role0:conj_and', 4)\n",
        "('role1:poss:role0:poss', 4)\n",
        "('role1:nsubjpass:role0:dobj', 4)\n",
        "('role1:nsubj:role0:dobj', 4)\n",
        "('role1:poss:role0:dep', 4)\n",
        "('role1:prep_to:role0:prep_to', 4)\n",
        "('role1:prep_from:role0:nsubj', 4)\n",
        "('role1:nn:role0:pobj', 4)\n",
        "('role1:prep_from:role0:prep_from', 4)\n",
        "('role1:nsubjpass:role0:nn', 4)\n",
        "('role1:prep_of:role0:conj_and', 4)\n",
        "('role1:nsubj:role0:pobj', 4)\n",
        "('role1:conj_and:role0:prep_of', 3)\n",
        "('role1:prep_from:role0:dep', 3)\n",
        "('role1:prep_on:role0:dep', 3)\n",
        "('role1:ccomp:role0:dep', 3)\n",
        "('role1:prep_in:role0:nsubj', 3)\n",
        "('role1:nsubjpass:role0:appos', 3)\n",
        "('role1:dep:role0:appos', 3)\n",
        "('role1:nn:role0:prep_about', 3)\n",
        "('role1:poss:role0:prep_in', 3)\n",
        "('role1:dobj:role0:prep_of', 3)\n",
        "('role1:prep_in:role0:appos', 3)\n",
        "('role1:appos:role0:nsubj', 3)\n",
        "('role1:nsubj:role0:prep_on', 3)\n",
        "('role1:nsubj:role0:tmod', 3)\n",
        "('role1:pobj:role0:nn', 3)\n",
        "('role1:prep_after:role0:nsubj', 3)\n",
        "('role1:nn:role0:prep_of', 3)\n",
        "('role1:nn:role0:prep_to', 2)\n",
        "('role1:dobj:role0:nsubjpass', 2)\n",
        "('role1:prep_as:role0:prep_as', 2)\n",
        "('role1:nsubj:role0:prep_by', 2)\n",
        "('role1:nn:role0:prep_across', 2)\n",
        "('role1:prep_by:role0:prep_by', 2)\n",
        "('role1:nsubjpass:role0:prep_of', 2)\n",
        "('role1:prep_through:role0:prep_through', 2)\n",
        "('role1:prep_of:role0:prep_with', 2)\n",
        "('role1:prep_with:role0:nn', 2)\n",
        "('role1:conj_or:role0:appos', 2)\n",
        "('role1:prep_between:role0:dobj', 2)\n",
        "('role1:parataxis:role0:parataxis', 2)\n",
        "('role1:poss:role0:prep_with', 2)\n",
        "('role1:nsubjpass:role0:root', 2)\n",
        "('role1:dobj:role0:tmod', 2)\n",
        "('role1:dep:role0:root', 2)\n",
        "('role1:nsubjpass:role0:conj_and', 2)\n",
        "('role1:appos:role0:prep_of', 2)\n",
        "('role1:prep_off:role0:prep_off', 2)\n",
        "('role1:prep_of:role0:prep_for', 2)\n",
        "('role1:amod:role0:nn', 2)\n",
        "('role1:prep_of:role0:root', 2)\n",
        "('role1:prep_throughout:role0:prep_throughout', 2)\n",
        "('role1:conj_or:role0:dep', 2)\n",
        "('role1:nn:role0:nsubj', 2)\n",
        "('role1:appos:role0:poss', 2)\n",
        "('role1:dep:role0:dobj', 2)\n",
        "('role1:poss:role0:nsubj', 2)\n",
        "('role1:advmod:role0:nn', 2)\n",
        "('role1:nn:role0:root', 2)\n",
        "('role1:advmod:role0:dobj', 2)\n",
        "('role1:root:role0:prep_with', 2)\n",
        "('role1:poss:role0:appos', 2)\n",
        "('role1:agent:role0:nn', 2)\n",
        "('role1:conj_and:role0:appos', 2)\n",
        "('role1:prep_in:role0:prep_with', 2)\n",
        "('role1:prep_from:role0:nn', 2)\n",
        "('role1:prep_about:role0:nsubj', 2)\n",
        "('role1:dobj:role0:root', 2)\n",
        "('role1:prep_for:role0:root', 2)\n",
        "('role1:conj_and:role0:prep_on', 1)\n",
        "('role1:nsubj:role0:prep_in', 1)\n",
        "('role1:conj_only:role0:dep', 1)\n",
        "('role1:dep:role0:conj_and', 1)\n",
        "('role1:poss:role0:prep_across', 1)\n",
        "('role1:prep_after:role0:prep_by', 1)\n",
        "('role1:xcomp:role0:dobj', 1)\n",
        "('role1:prep_out_of:role0:prep_out_of', 1)\n",
        "('role1:appos:role0:prep_with', 1)\n",
        "('role1:conj_but:role0:conj_but', 1)\n",
        "('role1:conj_or:role0:conj_and', 1)\n",
        "('role1:prep_during:role0:prep_on', 1)\n",
        "('role1:advmod:role0:root', 1)\n",
        "('role1:prep_with:role0:dep', 1)\n",
        "('role1:conj_and:role0:prep_throughout', 1)\n",
        "('role1:prep_with:role0:nsubj', 1)\n",
        "('role1:amod:role0:appos', 1)\n",
        "('role1:poss:role0:prep_of', 1)\n",
        "('role1:prep_by:role0:nsubj', 1)\n",
        "('role1:prep_between:role0:nn', 1)\n",
        "('role1:poss:role0:prep_on', 1)\n",
        "('role1:prep_to:role0:conj_or', 1)\n",
        "('role1:rcmod:role0:prep_with', 1)\n",
        "('role1:prep_between:role0:appos', 1)\n",
        "('role1:prep_of:role0:prep_to', 1)\n",
        "('role1:dobj:role0:prep_across', 1)\n",
        "('role1:nn:role0:prep_with', 1)\n",
        "('role1:prep_between:role0:prep_between', 1)\n",
        "('role1:conj_and:role0:dep', 1)\n",
        "('role1:nsubj:role0:prep_for', 1)\n",
        "('role1:dep:role0:prep_on', 1)\n",
        "('role1:ccomp:role0:nn', 1)\n",
        "('role1:iobj:role0:nn', 1)\n",
        "('role1:prep_in:role0:prep_on', 1)\n",
        "('role1:tmod:role0:root', 1)\n",
        "('role1:prep_during:role0:dep', 1)\n",
        "('role1:pobj:role0:poss', 1)\n",
        "('role1:agent:role0:appos', 1)\n",
        "('role1:prep_after:role0:dep', 1)\n",
        "('role1:prep_as:role0:dobj', 1)\n",
        "('role1:tmod:role0:prep_to', 1)\n",
        "('role1:prep_over:role0:nsubj', 1)\n",
        "('role1:prep_off:role0:dobj', 1)\n",
        "('role1:conj_or:role0:nsubj', 1)\n",
        "('role1:conj_or:role0:prep_on', 1)\n",
        "('role1:num:role0:npadvmod', 1)\n",
        "('role1:nsubjpass:role0:pobj', 1)\n",
        "('role1:conj_or:role0:nn', 1)\n",
        "('role1:prep_near:role0:prep_near', 1)\n",
        "('role1:nsubj:role0:rcmod', 1)\n",
        "('role1:prep_to:role0:prep_with', 1)\n",
        "('role1:prep_to:role0:nn', 1)\n",
        "('role1:rcmod:role0:rcmod', 1)\n",
        "('role1:dobj:role0:prep_on', 1)\n",
        "('role1:xcomp:role0:nn', 1)\n",
        "('role1:prep_with:role0:conj_and', 1)\n",
        "('role1:agent:role0:prep_of', 1)\n",
        "('role1:nn:role0:poss', 1)\n",
        "('role1:prep_on:role0:pobj', 1)\n",
        "('role1:prep_after:role0:prep_of', 1)\n",
        "('role1:prep_from:role0:poss', 1)\n",
        "('role1:appos:role0:prep_in', 1)\n",
        "('role1:prep_through:role0:nn', 1)\n",
        "('role1:prep_throughout:role0:nn', 1)\n",
        "('role1:prep_of:role0:tmod', 1)\n",
        "('role1:num:role0:dobj', 1)\n",
        "('role1:prep_for:role0:nn', 1)\n",
        "('role1:prep_across:role0:prep_across', 1)\n",
        "('role1:prep_about:role0:prep_for', 1)\n",
        "('role1:nn:role0:prep_in', 1)\n",
        "('role1:ccomp:role0:appos', 1)\n",
        "('role1:prep_in:role0:prep_to', 1)\n",
        "('role1:conj_or:role0:prep_to', 1)\n",
        "('role1:poss:role0:pobj', 1)\n",
        "('role1:appos:role0:dep', 1)\n",
        "('role1:xcomp:role0:xcomp', 1)\n",
        "('role1:prep_from:role0:dobj', 1)\n",
        "('role1:nsubj:role0:conj_or', 1)\n",
        "('role1:prep_after:role0:root', 1)\n",
        "('role1:prep_about:role0:nn', 1)\n",
        "('role1:appos:role0:prep_across', 1)\n",
        "('role1:prep_about:role0:dobj', 1)\n",
        "('role1:dobj:role0:prep_in', 1)\n",
        "('role1:prep_in:role0:conj_and', 1)\n",
        "('role1:prep_of:role0:pobj', 1)\n",
        "('role1:prep_of:role0:prep_in', 1)\n",
        "('role1:poss:role0:prep_from', 1)\n",
        "('role1:prep_in:role0:xcomp', 1)\n",
        "('role1:prep_near:role0:dobj', 1)\n",
        "('role1:prep_of:role0:prep_as', 1)\n",
        "('role1:prep_from:role0:pobj', 1)\n",
        "('role1:npadvmod:role0:npadvmod', 1)\n",
        "('role1:prep_as:role0:nn', 1)\n",
        "('role1:agent:role0:root', 1)\n",
        "('role1:num:role0:prep_of', 1)\n",
        "('role1:prep_for:role0:prep_for', 1)\n",
        "('role1:conj_and:role0:prep_with', 1)\n",
        "('role1:dep:role0:conj_or', 1)\n",
        "('role1:dobj:role0:prep_for', 1)\n",
        "('role1:dobj:role0:prep_about', 1)\n",
        "('role1:ccomp:role0:ccomp', 1)\n",
        "('role1:dep:role0:prep_with', 1)\n",
        "('role1:prep_about:role0:prep_about', 1)\n",
        "('role1:prep_from:role0:appos', 1)\n",
        "('role1:agent:role0:pobj', 1)\n",
        "('role1:rcmod:role0:nsubj', 1)\n",
        "('role1:tmod:role0:prep_in', 1)\n",
        "('role1:prep_throughout:role0:conj_and', 1)\n",
        "('role1:pobj:role0:prep_from', 1)\n",
        "('role1:xcomp:role0:nsubj', 1)\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n",
      "\n",
      "dataX = []\n",
      "dataY = []\n",
      "\n",
      "\n",
      "    doc = cnlp.Document(xml)\n",
      "    #if len(doc) < 8:\n",
      "    model = RushModel(doc, history=2)\n",
      "    gold_trans = model.gold_transitions()\n",
      "    dataX.append(model)\n",
      "    dataY.append(gold_trans)\n",
      "print len(dataX)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading data\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Role Labelings"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"running perceptron\"\n",
      "ptron.fit(dataX,dataY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running perceptron\n",
        "iteration 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "avg loss: 0.755618 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  1.  0.  0. -1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0. -1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  0.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 1\n",
        "avg loss: 0.750000 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0.  0.  0. -1.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  0.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 2\n",
        "avg loss: 0.738764 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  0.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 3\n",
        "avg loss: 0.750000 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -2.  0.  0.  0. -1.  1.  0.  0.  0.  3.  0.  0.  0. -1.  1.  0.  0.\n",
        "  -1.  0.  0.  0.  1.  0.  0.  0.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1. -1.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 4\n",
        "avg loss: 0.744382 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0. -1.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 5\n",
        "avg loss: 0.738764 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  0.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 6\n",
        "avg loss: 0.778090 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 7\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 8\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 9\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 10\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 11\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 12\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 13\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 14\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 15\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 16\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 17\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 18\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 19\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 20\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 21\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 22\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 23\n",
        "avg loss: 0.758427 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  1.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0. -1.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n",
        "iteration 24\n",
        "avg loss: 0.752809 w: [[ 0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  0.\n",
        "   0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.\n",
        "   0. -1.  0.  0.  0. -1.  1.  1.  0.  0.  3.  1.  0.  0.  0.  0.  0.  0.\n",
        "  -2.  0.  0.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.\n",
        "   0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
        "   0.  0.  0.  1.  1.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1.\n",
        "   0.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.\n",
        "   0.  0.  0.  1.  0.  0.  0. -2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.\n",
        "   1.  1.]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "effective learning rate: 1.000000\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"loading test data\"\n",
      "testX = []\n",
      "\n",
      "for xml in data.corenlp_apws_train():\n",
      "    doc = cnlp.Document(xml)\n",
      "    if len(doc) < 8:\n",
      "        model = RushModel(doc, history=2)\n",
      "        testX.append(model)\n",
      "        \n",
      "\n",
      "print len(testX)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading test data\n",
        "54"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Predicting...\"\n",
      "predY = ptron.predict(testX)\n",
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Predicting...\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import getenv\n",
      "from os.path import join\n",
      "\n",
      "pred_fname = '/tmp/predicted_file'\n",
      "pred_file = open(pred_fname, 'w')\n",
      "\n",
      "gold_fname = '/tmp/gold_file'\n",
      "gold_file = open(gold_fname, 'w')\n",
      "\n",
      "ddir = getenv(\"DISCOURSEDIR\",\".\")\n",
      "eval_script = join(ddir, \"eval\", \"ordering-eval.py\")\n",
      "\n",
      "for i, model in enumerate(testX):\n",
      "    pred_order = predY[i]\n",
      "    pred_str = model.ordering2str(pred_order)\n",
      "    \n",
      "    pred_file.write(pred_str+\"\\n\\n\")\n",
      "    pred_file.flush()\n",
      "    \n",
      "    gold_str = model.gold_str()\n",
      "    gold_file.write(gold_str+\"\\n\\n\")\n",
      "    gold_file.flush()\n",
      "\n",
      "pred_file.close()\n",
      "gold_file.close()\n",
      "\n",
      "!python $eval_script --gold $gold_fname --predicted $pred_fname     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total documents: 54\r\n",
        "Total correct: 0\r\n",
        "Accuracy: 0.0\r\n",
        "Avg. Kendall's Tau 0.222222222222\r\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "trenton\n",
        "jason\n",
        "aiden\n",
        "steve\n",
        "andre\n",
        "gabriel\n",
        "dalton\n",
        "liam\n",
        "jerome\n",
        "cody\n",
        "pedro\n",
        "aaron\n",
        "edward\n",
        "elias\n",
        "daryl\n",
        "jack\n",
        "jacob\n",
        "isaiah\n",
        "roy\n",
        "louis\n",
        "joel\n",
        "michael\n",
        "ryan\n",
        "herbert\n",
        "donovan\n",
        "harry\n",
        "guy\n",
        "leonardo\n",
        "carter\n",
        "ted\n",
        "cameron\n",
        "colin\n",
        "cesar\n",
        "seth\n",
        "clarence\n",
        "richard\n",
        "ruben\n",
        "nicholas\n",
        "christian\n",
        "lee\n",
        "henry\n",
        "drew\n",
        "jonah\n",
        "kaden\n",
        "luis\n",
        "terrence\n",
        "micah\n",
        "calvin\n",
        "omar\n",
        "bryan\n",
        "bob\n",
        "gene\n",
        "joshua\n",
        "aidan\n",
        "donnie\n",
        "clifford\n",
        "russell\n",
        "hector\n",
        "wayne\n",
        "jonathan\n",
        "rick\n",
        "kerry\n",
        "oscar\n",
        "leonard\n",
        "mathew\n",
        "floyd\n",
        "brayden\n",
        "phillip\n",
        "kristopher\n",
        "ernest\n",
        "kenneth\n",
        "dennis\n",
        "brian\n",
        "ronnie\n",
        "max\n",
        "brett\n",
        "ronald\n",
        "levi\n",
        "jonathon\n",
        "rickey\n",
        "danny\n",
        "brady\n",
        "leroy\n",
        "trevor\n",
        "gerard\n",
        "kurt\n",
        "juan\n",
        "emmanuel\n",
        "logan\n",
        "scott\n",
        "cole\n",
        "warren\n",
        "glen\n",
        "blake\n",
        "gerald\n",
        "jim\n",
        "ricardo\n",
        "tyrone\n",
        "tyler\n",
        "conner\n",
        "isaac\n",
        "bryson\n",
        "peyton\n",
        "eugene\n",
        "curtis\n",
        "dan\n",
        "hunter\n",
        "marcus\n",
        "damian\n",
        "james\n",
        "brody\n",
        "alexis\n",
        "billy\n",
        "carlos\n",
        "alec\n",
        "neil\n",
        "steven\n",
        "allan\n",
        "bradley\n",
        "alejandro\n",
        "todd\n",
        "randolph\n",
        "shannon\n",
        "rex\n",
        "perry\n",
        "jackson\n",
        "robert\n",
        "douglas\n",
        "mason\n",
        "hayden\n",
        "daniel\n",
        "kent\n",
        "jalen\n",
        "benjamin\n",
        "harold\n",
        "jeremiah\n",
        "lawrence\n",
        "frank\n",
        "fred\n",
        "darius\n",
        "johnnie\n",
        "carl\n",
        "randall\n",
        "philip\n",
        "keith\n",
        "david\n",
        "gordon\n",
        "adrian\n",
        "austin\n",
        "cristian\n",
        "jesus\n",
        "george\n",
        "timothy\n",
        "ian\n",
        "carson\n",
        "garry\n",
        "alfred\n",
        "tom\n",
        "gilbert\n",
        "john\n",
        "shawn\n",
        "jamie\n",
        "albert\n",
        "alan\n",
        "jordan\n",
        "milton\n",
        "nathan\n",
        "zachary\n",
        "roberto\n",
        "tommy\n",
        "greg\n",
        "terrance\n",
        "dana\n",
        "angel\n",
        "julian\n",
        "preston\n",
        "caleb\n",
        "allen\n",
        "kevin\n",
        "ray\n",
        "melvin\n",
        "brandon\n",
        "derek\n",
        "victor\n",
        "norman\n",
        "bruce\n",
        "johnathan\n",
        "freddie\n",
        "mario\n",
        "nolan\n",
        "connor\n",
        "tracy\n",
        "herman\n",
        "don\n",
        "frederick\n",
        "xavier\n",
        "abraham\n",
        "thomas\n",
        "parker\n",
        "lewis\n",
        "charles\n",
        "troy\n",
        "lloyd\n",
        "jared\n",
        "kaleb\n",
        "jay\n",
        "bernard\n",
        "randy\n",
        "colby\n",
        "ethan\n",
        "jermaine\n",
        "luke\n",
        "samuel\n",
        "christopher\n",
        "alexander\n",
        "earl\n",
        "brendan\n",
        "jaime\n",
        "roger\n",
        "vincent\n",
        "maurice\n",
        "dillon\n",
        "travis\n",
        "leslie\n",
        "sean\n",
        "stanley\n",
        "braden\n",
        "lonnie\n",
        "harvey\n",
        "ralph\n",
        "gage\n",
        "nathaniel\n",
        "tristan\n",
        "marvin\n",
        "duane\n",
        "clayton\n",
        "vernon\n",
        "elijah\n",
        "joe\n",
        "leo\n",
        "jon\n",
        "javier\n",
        "colton\n",
        "darin\n",
        "edgar\n",
        "darryl\n",
        "brad\n",
        "rodney\n",
        "jeff\n",
        "leon\n",
        "tony\n",
        "devin\n",
        "oliver\n",
        "chad\n",
        "howard\n",
        "barry\n",
        "darren\n",
        "mark\n",
        "geoffrey\n",
        "craig\n",
        "riley\n",
        "walter\n",
        "jaden\n",
        "mike\n",
        "andres\n",
        "andrew\n",
        "anthony\n",
        "stephen\n",
        "dave\n",
        "bill\n",
        "arnold\n",
        "wyatt\n",
        "micheal\n",
        "dominic\n",
        "mitchell\n",
        "robin\n",
        "owen\n",
        "jackie\n",
        "marco\n",
        "darrell\n",
        "chase\n",
        "ayden\n",
        "grant\n",
        "edwin\n",
        "jerry\n",
        "matthew\n",
        "karl\n",
        "damon\n",
        "franklin\n",
        "jeffrey\n",
        "raymond\n",
        "theodore\n",
        "kim\n",
        "jayden\n",
        "dylan\n",
        "bryce\n",
        "lucas\n",
        "charlie\n",
        "reginald\n",
        "kyle\n",
        "antonio\n",
        "kelly\n",
        "spencer\n",
        "alvin\n",
        "noah\n",
        "gavin\n",
        "lester\n",
        "dakota\n",
        "paul\n",
        "nicolas\n",
        "jose\n",
        "eli\n",
        "tim\n",
        "clyde\n",
        "ross\n",
        "jake\n",
        "gregg\n",
        "alex\n",
        "ricky\n",
        "bobby\n",
        "patrick\n",
        "fernando\n",
        "arthur\n",
        "martin\n",
        "ivan\n",
        "sebastian\n",
        "dale\n",
        "dean\n",
        "eddie\n",
        "peter\n",
        "jeffery\n",
        "brent\n",
        "jimmie\n",
        "cooper\n",
        "garrett\n",
        "willie\n",
        "jorge\n",
        "wesley\n",
        "francis\n",
        "cory\n",
        "justin\n",
        "marc\n",
        "stuart\n",
        "gregory\n",
        "dwight\n",
        "jimmy\n",
        "clinton\n",
        "glenn\n",
        "maxwell\n",
        "sergio\n",
        "gary\n",
        "caden\n",
        "dwayne\n",
        "joseph\n",
        "shane\n",
        "larry\n",
        "josiah\n",
        "lynn\n",
        "erick\n",
        "donald\n",
        "miguel\n",
        "malik\n",
        "manuel\n",
        "eduardo\n",
        "taylor\n",
        "johnny\n",
        "erik\n",
        "eric\n",
        "jesse\n",
        "diego\n",
        "landon\n",
        "collin\n",
        "kirk\n",
        "corey\n",
        "malachi\n",
        "giovanni\n",
        "lance\n",
        "shaun\n",
        "francisco\n",
        "dustin\n",
        "william\n",
        "devon\n",
        "evan\n",
        "terry\n",
        "chris\n",
        "jeremy\n",
        "casey\n",
        "tanner\n",
        "derrick\n",
        "adam\n",
        "ashton\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for doc in train_apws:\n",
      "\n",
      "    \n",
      "    #doc = train_ntsb[11]\n",
      "for s in doc:\n",
      "    for t in s:\n",
      "        if 'PERSON' in t.ne:\n",
      "            print t.lem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = defaultdict(int)\n",
      "\n",
      "for doc in train_apws:\n",
      "    model = RushModel(doc, history=2, features=set(['pron_res']))\n",
      "    gold_trans = model.gold_transitions()\n",
      "    for trans in gold_trans:\n",
      "        for f in model.feature_map(trans):\n",
      "            counts[f] += 1\n",
      "\n",
      "fcounts = [(f, counts[f]) for f in counts]\n",
      "fcounts.sort(reverse=True, key=lambda a:a[1])\n",
      "for c in fcounts:\n",
      "    print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('he_person', 32)\n",
        "('it_org', 26)\n",
        "('it_person', 17)\n",
        "('he_org', 13)\n",
        "('its_org', 9)\n",
        "('they_person', 9)\n",
        "('i_person', 7)\n",
        "('she_person', 5)\n",
        "('we_person', 5)\n",
        "('it_male', 4)\n",
        "('they_org', 4)\n",
        "('himself_org', 4)\n",
        "('he_male', 3)\n",
        "('she_org', 3)\n",
        "('herself_org', 2)\n",
        "('himself_person', 2)\n",
        "('my_person', 2)\n",
        "('we_org', 1)\n",
        "('he_female', 1)\n",
        "('you_person', 1)\n",
        "('you_male', 1)\n",
        "('it_female', 1)\n",
        "('i_male', 1)\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = defaultdict(int)\n",
      "\n",
      "for doc in train_apws:\n",
      "    model = RushModel(doc, history=2, features=set(['ne_tags']))\n",
      "    gold_trans = model.gold_transitions()\n",
      "    for trans in gold_trans:\n",
      "        for f in model.feature_map(trans):\n",
      "            counts[f] += 1\n",
      "\n",
      "fcounts = [(f, counts[f]) for f in counts]\n",
      "fcounts.sort(reverse=True, key=lambda a:a[1])\n",
      "for c in fcounts:\n",
      "    print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('-s0_PERS_c0-s1_LOCS_c2', 142)\n",
        "('-s0_PERS_c0-s1_LOCS_c1', 136)\n",
        "('-s0_LOCS_c1-s1_PERS_c0', 124)\n",
        "('-s0_LOCS_c2-s1_PERS_c0', 124)\n",
        "('-s0_ORGS_c0-s1_LOCS_c1', 122)\n",
        "('-s0_ORGS_c0-s1_LOCS_c2', 112)\n",
        "('-s0_PERS_c0', 99)\n",
        "('-s0_LOCS_c2-s1_ORGS_c0', 96)\n",
        "('-s1_ORGS_c0', 96)\n",
        "('-s0_LOCS_c1-s1_ORGS_c0', 95)\n",
        "('-s0_PERS_c0-s1_ORGS_c1', 95)\n",
        "('-s1_PERS_c0', 95)\n",
        "('-s0_ORGS_c1-s1_PERS_c0', 89)\n",
        "('-s0_LOCS_c0-s1_LOCS_c1', 88)\n",
        "('-s0_ORGS_c1-s1_ORGS_c0', 87)\n",
        "('-s0_ORGS_c0-s1_ORGS_c1', 78)\n",
        "('-s0_ORGS_c0', 72)\n",
        "('-s1_LOCS_c0', 72)\n",
        "('-s0_LOCS_c3-s1_PERS_c0', 68)\n",
        "('-s0_PERS_c0-s1_LOCS_c3', 67)\n",
        "('-s0_LOCS_c0-s1_LOCS_c2', 65)\n",
        "('-s0_LOCS_c3-s1_ORGS_c0', 61)\n",
        "('-s0_ORGS_c0-s1_LOCS_c3', 61)\n",
        "('-s0_LOCS_c1-s1_LOCS_c0', 56)\n",
        "('-s0_LOCS_c0-s1_ORGS_c1', 55)\n",
        "('-s0_ORGS_c3-s1_PERS_c0', 54)\n",
        "('-s0_PERS_c0-s1_PERS_c2', 51)\n",
        "('-s0_ORGS_c0-s1_ORGS_c3', 51)\n",
        "('-s0_PERS_c2-s1_PERS_c0', 50)\n",
        "('-s0_LOCS_c2-s1_LOCS_c0', 49)\n",
        "('-s0_PERS_c0-s1_LOCS_c4', 46)\n",
        "('-s0_ORGS_c1-s1_LOCS_c0', 46)\n",
        "('-s0_ORGS_c0-s1_PERS_c2', 45)\n",
        "('-s0_PERS_c0-s1_ORGS_c3', 44)\n",
        "('-s0_ORGS_c0-s1_PERS_c1', 41)\n",
        "('-s0_PERS_c1-s1_PERS_c0', 39)\n",
        "('-s0_LOCS_c0-s1_LOCS_c3', 38)\n",
        "('-s0_PERS_c2-s1_ORGS_c0', 35)\n",
        "('-s0_PERS_c0-s1_PERS_c1', 34)\n",
        "('-s0_LOCS_c3-s1_LOCS_c0', 33)\n",
        "('-s0_LOCS_c1-s1_LOCS_c2', 33)\n",
        "('-s0_ORGS_c3-s1_ORGS_c0', 32)\n",
        "('-s0_LOCS_c4-s1_PERS_c0', 31)\n",
        "('-s0_PERS_c1-s1_ORGS_c0', 30)\n",
        "('-s0_ORGS_c0-s1_LOCS_c4', 29)\n",
        "('-s0_LOCS_c5-s1_PERS_c0', 29)\n",
        "('-s0_LOCS_c0-s1_PERS_c2', 28)\n",
        "('-s0_PERS_c2-s1_LOCS_c0', 27)\n",
        "('-s0_PERS_c1-s1_LOCS_c0', 27)\n",
        "('-s0_LOCS_c2-s1_LOCS_c1', 27)\n",
        "('-s0_LOCS_c1', 27)\n",
        "('-s0_ORGS_c0-s1_LOCS_c5', 26)\n",
        "('-s0_LOCS_c0-s1_PERS_c1', 26)\n",
        "('-s0_PERS_c0-s1_LOCS_c5', 26)\n",
        "('-s0_LOCS_c4-s1_ORGS_c0', 26)\n",
        "('-s0_PERS_c0-s1_ORGS_c4', 25)\n",
        "('-s0_ORGS_c0-s1_ORGS_c4', 25)\n",
        "('-s0_ORGS_c4-s1_PERS_c0', 24)\n",
        "('-s0_LOCS_c2-s1_LOCS_c2', 23)\n",
        "('-s0_LOCS_c2', 23)\n",
        "('-s0_ORGS_c1-s1_LOCS_c1', 22)\n",
        "('-s0_LOCS_c5-s1_ORGS_c0', 22)\n",
        "('-s0_ORGS_c3-s1_LOCS_c0', 22)\n",
        "('-s0_LOCS_c1-s1_ORGS_c1', 21)\n",
        "('-s0_LOCS_c1-s1_LOCS_c1', 21)\n",
        "('-s0_ORGS_c1-s1_LOCS_c2', 20)\n",
        "('-s0_ORGS_c3-s1_ORGS_c1', 20)\n",
        "('-s0_ORGS_c1', 19)\n",
        "('-s0_LOCS_c0-s1_ORGS_c4', 19)\n",
        "('-s0_LOCS_c0-s1_ORGS_c3', 19)\n",
        "('-s0_LOCS_c0', 19)\n",
        "('-s0_LOCS_c2-s1_ORGS_c1', 18)\n",
        "('-s0_LOCS_c4', 18)\n",
        "('-s0_PERS_c0-s1_ORGS_c2', 17)\n",
        "('-s0_LOCS_c0-s1_LOCS_c4', 16)\n",
        "('-s0_PERS_c2-s1_ORGS_c1', 15)\n",
        "('-s0_LOCS_c1-s1_ORGS_c3', 15)\n",
        "('-s0_LOCS_c2-s1_LOCS_c3', 15)\n",
        "('-s0_LOCS_c2-s1_ORGS_c3', 15)\n",
        "('-s0_ORGS_c4-s1_ORGS_c0', 15)\n",
        "('-s0_LOCS_c1-s1_LOCS_c4', 15)\n",
        "('-s0_ORGS_c3-s1_LOCS_c4', 15)\n",
        "('-s0_ORGS_c0-s1_ORGS_c2', 15)\n",
        "('-s1_LOCS_c1', 15)\n",
        "('-s0_PERS_c0-s1_LOCS_clots', 15)\n",
        "('-s0_LOCS_c2-s1_LOCS_c4', 14)\n",
        "('-s0_LOCS_c5-s1_LOCS_c0', 14)\n",
        "('-s0_ORGS_c0-s1_LOCS_clots', 14)\n",
        "('-s0_ORGS_c2-s1_PERS_c0', 13)\n",
        "('-s0_LOCS_c3-s1_LOCS_c2', 13)\n",
        "('-s0_LOCS_c4-s1_LOCS_c0', 13)\n",
        "('-s0_ORGS_c2-s1_ORGS_c0', 13)\n",
        "('-s0_LOCS_c1-s1_PERS_c2', 12)\n",
        "('-s0_PERS_c2-s1_LOCS_c1', 12)\n",
        "('-s1_LOCS_c2', 11)\n",
        "('-s0_LOCS_c3-s1_LOCS_c3', 11)\n",
        "('-s0_PERS_c2-s1_LOCS_c3', 10)\n",
        "('-s0_LOCS_clots-s1_PERS_c0', 10)\n",
        "('-s0_LOCS_c1-s1_LOCS_c3', 10)\n",
        "('-s0_LOCS_c0-s1_LOCS_clots', 9)\n",
        "('-s0_ORGS_c1-s1_LOCS_c3', 9)\n",
        "('-s0_ORGS_c1-s1_PERS_c2', 9)\n",
        "('-s0_PERS_c3-s1_PERS_c0', 9)\n",
        "('-s0_LOCS_c2-s1_PERS_c1', 9)\n",
        "('-s0_LOCS_c3-s1_LOCS_c1', 9)\n",
        "('-s0_LOCS_c4-s1_LOCS_c2', 9)\n",
        "('-s0_PERS_c0-s1_PERS_c3', 9)\n",
        "('-s0_LOCS_c4-s1_ORGS_c1', 9)\n",
        "('-s0_PERS_c1-s1_ORGS_c1', 9)\n",
        "('-s0_ORGS_c1-s1_ORGS_c1', 9)\n",
        "('-s0_LOCS_c0-s1_LOCS_c5', 8)\n",
        "('-s0_LOCS_c3-s1_ORGS_c1', 8)\n",
        "('-s0_PERS_c1-s1_LOCS_c1', 8)\n",
        "('-s0_LOCS_c2-s1_LOCS_c5', 8)\n",
        "('-s0_LOCS_c1-s1_LOCS_c5', 8)\n",
        "('-s0_LOCS_c3-s1_PERS_c2', 8)\n",
        "('-s0_PERS_c1-s1_ORGS_c3', 8)\n",
        "('-s0_PERS_c3-s1_ORGS_c0', 8)\n",
        "('-s0_LOCS_c1-s1_PERS_c1', 7)\n",
        "('-s0_LOCS_c0-s1_PERS_c3', 7)\n",
        "('-s0_ORGS_c0-s1_PERS_c3', 7)\n",
        "('-s0_LOCS_c4-s1_LOCS_c1', 7)\n",
        "('-s0_LOCS_c0-s1_ORGS_c2', 7)\n",
        "('-s0_LOCS_c5-s1_LOCS_c2', 7)\n",
        "('-s0_ORGS_c4-s1_LOCS_c3', 6)\n",
        "('-s0_ORGS_c4-s1_LOCS_c2', 6)\n",
        "('-s0_LOCS_c5-s1_ORGS_c1', 6)\n",
        "('-s0_LOCS_c1-s1_ORGS_c2', 6)\n",
        "('-s0_LOCS_c2-s1_PERS_c2', 6)\n",
        "('-s0_LOCS_c3-s1_LOCS_c5', 6)\n",
        "('-s0_LOCS_clots-s1_ORGS_c0', 6)\n",
        "('-s0_ORGS_c4-s1_ORGS_c1', 6)\n",
        "('-s0_LOCS_clots', 6)\n",
        "('-s0_LOCS_clots-s1_LOCS_c0', 6)\n",
        "('-s0_PERS_c2-s1_PERS_c1', 6)\n",
        "('-s0_PERS_c1-s1_PERS_c1', 6)\n",
        "('-s0_ORGS_c3-s1_LOCS_c2', 6)\n",
        "('-s0_ORGS_c3-s1_LOCS_c1', 6)\n",
        "('-s0_LOCS_c3', 6)\n",
        "('-s0_ORGS_c0-s1_ORGS_c5', 6)\n",
        "('-s0_ORGS_c2-s1_LOCS_c0', 6)\n",
        "('-s0_ORGS_c5-s1_PERS_c0', 6)\n",
        "('-s0_PERS_c0-s1_ORGS_c5', 5)\n",
        "('-s0_ORGS_c4-s1_LOCS_c1', 5)\n",
        "('-s0_ORGS_c1-s1_LOCS_c4', 5)\n",
        "('-s0_PERS_c2-s1_LOCS_c2', 5)\n",
        "('-s0_PERS_c2-s1_LOCS_c4', 5)\n",
        "('-s0_ORGS_c4', 5)\n",
        "('-s0_LOCS_c3-s1_LOCS_c4', 5)\n",
        "('-s0_LOCS_c2-s1_ORGS_c4', 5)\n",
        "('-s0_ORGS_c5-s1_ORGS_c0', 5)\n",
        "('-s0_LOCS_c4-s1_LOCS_c3', 5)\n",
        "('-s0_ORGS_c3-s1_LOCS_c3', 5)\n",
        "('-s0_ORGS_c2-s1_LOCS_c2', 5)\n",
        "('-s0_LOCS_c1-s1_ORGS_c4', 4)\n",
        "('-s0_LOCS_c5-s1_LOCS_c3', 4)\n",
        "('-s0_LOCS_c3-s1_ORGS_c3', 4)\n",
        "('-s0_PERS_c1-s1_LOCS_c5', 4)\n",
        "('-s0_PERS_c1-s1_LOCS_c3', 4)\n",
        "('-s0_PERS_c1-s1_LOCS_c2', 4)\n",
        "('-s0_LOCS_c4-s1_PERS_c2', 4)\n",
        "('-s0_LOCS_c2-s1_ORGS_c2', 4)\n",
        "('-s0_PERS_c3-s1_LOCS_c3', 4)\n",
        "('-s0_PERS_c3-s1_LOCS_c0', 4)\n",
        "('-s0_ORGS_c2-s1_PERS_c2', 4)\n",
        "('-s0_ORGS_c4-s1_LOCS_c0', 4)\n",
        "('-s0_ORGS_c2-s1_LOCS_c3', 4)\n",
        "('-s0_ORGS_c1-s1_ORGS_c2', 4)\n",
        "('-s0_LOCS_c5-s1_LOCS_c1', 3)\n",
        "('-s0_PERS_c2-s1_ORGS_c2', 3)\n",
        "('-s0_PERS_c2-s1_ORGS_c3', 3)\n",
        "('-s0_LOCS_c1-s1_ORGS_c5', 3)\n",
        "('-s0_LOCS_c3-s1_ORGS_c5', 3)\n",
        "('-s0_ORGS_c3-s1_ORGS_c4', 3)\n",
        "('-s0_ORGS_c1-s1_PERS_c3', 3)\n",
        "('-s0_ORGS_clots-s1_PERS_c0', 3)\n",
        "('-s0_ORGS_c5-s1_LOCS_c0', 3)\n",
        "('-s0_ORGS_c3-s1_LOCS_c5', 3)\n",
        "('-s0_ORGS_c2', 3)\n",
        "('-s1_ORGS_c1', 3)\n",
        "('-s0_PERS_c2-s1_PERS_c2', 3)\n",
        "('-s0_PERS_c1-s1_PERS_c2', 3)\n",
        "('-s0_ORGS_c2-s1_ORGS_c1', 3)\n",
        "('-s1_PERS_c1', 3)\n",
        "('-s0_LOCS_c2-s1_LOCS_clots', 3)\n",
        "('-s0_PERS_c2-s1_ORGS_c4', 2)\n",
        "('-s0_LOCS_c5-s1_LOCS_c4', 2)\n",
        "('-s0_ORGS_c4-s1_LOCS_c4', 2)\n",
        "('-s1_LOCS_c4', 2)\n",
        "('-s0_PERS_c1-s1_LOCS_c4', 2)\n",
        "('-s1_PERS_c2', 2)\n",
        "('-s0_LOCS_c4-s1_PERS_c1', 2)\n",
        "('-s0_LOCS_clots-s1_ORGS_c2', 2)\n",
        "('-s0_ORGS_c4-s1_ORGS_c3', 2)\n",
        "('-s0_LOCS_clots-s1_LOCS_c3', 2)\n",
        "('-s0_LOCS_c4-s1_LOCS_c4', 2)\n",
        "('-s0_PERS_c0-s1_ORGS_clots', 2)\n",
        "('-s0_ORGS_c0-s1_ORGS_clots', 2)\n",
        "('-s0_ORGS_clots-s1_ORGS_c1', 2)\n",
        "('-s0_LOCS_c3-s1_PERS_c1', 2)\n",
        "('-s0_LOCS_c3-s1_LOCS_clots', 2)\n",
        "('-s0_LOCS_clots-s1_ORGS_c1', 2)\n",
        "('-s0_LOCS_c4-s1_ORGS_c3', 2)\n",
        "('-s0_ORGS_c3-s1_PERS_c1', 2)\n",
        "('-s0_ORGS_c1-s1_PERS_c1', 2)\n",
        "('-s0_PERS_c0-s1_PERS_clots', 1)\n",
        "('-s0_PERS_c2-s1_ORGS_c5', 1)\n",
        "('-s0_LOCS_c3-s1_ORGS_c4', 1)\n",
        "('-s0_LOCS_c0-s1_PERS_clots', 1)\n",
        "('-s0_ORGS_c1-s1_ORGS_c4', 1)\n",
        "('-s0_PERS_c3-s1_LOCS_clots', 1)\n",
        "('-s0_ORGS_c4-s1_LOCS_c5', 1)\n",
        "('-s0_PERS_c2-s1_PERS_c5', 1)\n",
        "('-s0_LOCS_c5-s1_PERS_c2', 1)\n",
        "('-s0_PERS_c5-s1_PERS_c2', 1)\n",
        "('-s0_ORGS_c2-s1_PERS_c1', 1)\n",
        "('-s0_PERS_clots-s1_LOCS_c0', 1)\n",
        "('-s0_PERS_c2-s1_LOCS_c5', 1)\n",
        "('-s0_ORGS_c1-s1_ORGS_c3', 1)\n",
        "('-s0_ORGS_c2-s1_ORGS_clots', 1)\n",
        "('-s0_LOCS_c3-s1_ORGS_c2', 1)\n",
        "('-s0_PERS_c3-s1_PERS_c2', 1)\n",
        "('-s0_PERS_c1-s1_PERS_c3', 1)\n",
        "('-s0_PERS_c5-s1_LOCS_c0', 1)\n",
        "('-s0_LOCS_c4-s1_LOCS_clots', 1)\n",
        "('-s0_PERS_c5-s1_ORGS_c3', 1)\n",
        "('-s0_ORGS_c5', 1)\n",
        "('-s0_LOCS_c4-s1_PERS_c3', 1)\n",
        "('-s0_ORGS_c1-s1_LOCS_clots', 1)\n",
        "('-s0_LOCS_c0-s1_PERS_c5', 1)\n",
        "('-s0_PERS_c2-s1_ORGS_clots', 1)\n",
        "('-s0_ORGS_clots-s1_LOCS_c0', 1)\n",
        "('-s0_ORGS_clots-s1_LOCS_c2', 1)\n",
        "('-s0_ORGS_c2-s1_LOCS_c4', 1)\n",
        "('-s0_ORGS_c0-s1_PERS_c5', 1)\n",
        "('-s0_ORGS_c4-s1_ORGS_c2', 1)\n",
        "('-s0_PERS_c2', 1)\n",
        "('-s0_ORGS_c5-s1_ORGS_c1', 1)\n",
        "('-s0_ORGS_c0-s1_PERS_clots', 1)\n",
        "('-s1_ORGS_c3', 1)\n",
        "('-s0_ORGS_c5-s1_LOCS_clots', 1)\n",
        "('-s0_LOCS_clots-s1_LOCS_c1', 1)\n",
        "('-s0_LOCS_clots-s1_LOCS_c2', 1)\n",
        "('-s0_PERS_c3-s1_LOCS_c4', 1)\n",
        "('-s0_ORGS_c5-s1_LOCS_c2', 1)\n",
        "('-s0_LOCS_c4-s1_LOCS_c5', 1)\n",
        "('-s0_LOCS_c2-s1_ORGS_clots', 1)\n",
        "('-s0_LOCS_c0-s1_ORGS_c5', 1)\n",
        "('-s0_LOCS_c3-s1_ORGS_clots', 1)\n",
        "('-s0_ORGS_c5-s1_LOCS_c4', 1)\n",
        "('-s0_PERS_c1-s1_ORGS_c4', 1)\n",
        "('-s0_ORGS_clots-s1_ORGS_c0', 1)\n",
        "('-s0_LOCS_c5-s1_ORGS_c3', 1)\n",
        "('-s0_LOCS_c5', 1)\n",
        "('-s0_LOCS_c1-s1_LOCS_clots', 1)\n",
        "('-s0_LOCS_c3-s1_PERS_c3', 1)\n",
        "('-s0_LOCS_c1-s1_PERS_c3', 1)\n",
        "('-s0_PERS_clots-s1_ORGS_c0', 1)\n",
        "('-s0_ORGS_c1-s1_ORGS_c5', 1)\n",
        "('-s0_ORGS_c2-s1_LOCS_c1', 1)\n",
        "('-s0_ORGS_c2-s1_LOCS_c5', 1)\n",
        "('-s0_LOCS_c4-s1_ORGS_c2', 1)\n",
        "('-s0_ORGS_c2-s1_ORGS_c3', 1)\n",
        "('-s0_PERS_c1-s1_ORGS_c2', 1)\n",
        "('-s0_PERS_clots-s1_PERS_c0', 1)\n",
        "('-s0_ORGS_c3-s1_ORGS_c2', 1)\n",
        "('-s0_ORGS_c3-s1_PERS_c2', 1)\n",
        "('-s0_ORGS_clots-s1_LOCS_c4', 1)\n",
        "('-s0_LOCS_c5-s1_ORGS_clots', 1)\n",
        "('-s0_ORGS_c3-s1_ORGS_c3', 1)\n",
        "('-s0_PERS_c3-s1_ORGS_c5', 1)\n",
        "('-s0_PERS_c3-s1_ORGS_c4', 1)\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = defaultdict(int)\n",
      "\n",
      "for doc in train_apws:\n",
      "    model = RushModel(doc, history=2, features=set(['num_caps']))\n",
      "    gold_trans = model.gold_transitions()\n",
      "    for trans in gold_trans:\n",
      "        for f in model.feature_map(trans):\n",
      "            counts[f] += 1\n",
      "\n",
      "fcounts = [(f, counts[f]) for f in counts]\n",
      "fcounts.sort(reverse=True, key=lambda a:a[1])\n",
      "for c in fcounts:\n",
      "    print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('-s0_caps_c_2-s1_caps_c_0', 70)\n",
        "('-s0_caps_c_0-s1_caps_c_2', 65)\n",
        "('-s0_caps_c_0-s1_caps_c_1', 56)\n",
        "('-s0_caps_c_1-s1_caps_c_0', 48)\n",
        "('-s1_caps_c_0', 47)\n",
        "('-s0_caps_c_5-s1_caps_c_0', 41)\n",
        "('-s0_caps_c_0', 38)\n",
        "('-s1_caps_c_2', 37)\n",
        "('-s0_caps_c_0-s1_caps_c_5', 30)\n",
        "('-s0_caps_c_1', 30)\n",
        "('-s0_caps_c_2-s1_caps_c_5', 24)\n",
        "('-s0_caps_c_0-s1_caps_c_3', 23)\n",
        "('-s0_caps_c_0-s1_caps_c_4', 22)\n",
        "('-s0_caps_c_4-s1_caps_c_0', 22)\n",
        "('-s0_caps_c_0-s1_caps_c_lots', 17)\n",
        "('-s0_caps_c_4', 15)\n",
        "('-s0_caps_c_3-s1_caps_c_0', 14)\n",
        "('-s0_caps_c_5-s1_caps_c_1', 12)\n",
        "('-s0_caps_c_3-s1_caps_c_5', 11)\n",
        "('-s1_caps_c_1', 11)\n",
        "('-s0_caps_c_2-s1_caps_c_2', 11)\n",
        "('-s0_caps_c_1-s1_caps_c_1', 10)\n",
        "('-s0_caps_c_2-s1_caps_c_4', 10)\n",
        "('-s0_caps_c_5', 9)\n",
        "('-s0_caps_c_2-s1_caps_c_1', 9)\n",
        "('-s0_caps_c_lots-s1_caps_c_0', 9)\n",
        "('-s0_caps_c_5-s1_caps_c_2', 8)\n",
        "('-s0_caps_c_3-s1_caps_c_2', 6)\n",
        "('-s0_caps_c_4-s1_caps_c_2', 6)\n",
        "('-s0_caps_c_5-s1_caps_c_4', 6)\n",
        "('-s0_caps_c_1-s1_caps_c_5', 6)\n",
        "('-s0_caps_c_4-s1_caps_c_1', 6)\n",
        "('-s0_caps_c_2-s1_caps_c_3', 6)\n",
        "('-s0_caps_c_1-s1_caps_c_4', 5)\n",
        "('-s0_caps_c_4-s1_caps_c_5', 5)\n",
        "('-s0_caps_c_2', 5)\n",
        "('-s1_caps_c_4', 5)\n",
        "('-s0_caps_c_lots-s1_caps_c_4', 5)\n",
        "('-s0_caps_c_3-s1_caps_c_4', 4)\n",
        "('-s0_caps_c_1-s1_caps_c_3', 4)\n",
        "('-s0_caps_c_4-s1_caps_c_4', 3)\n",
        "('-s0_caps_c_4-s1_caps_c_3', 3)\n",
        "('-s0_caps_c_5-s1_caps_c_5', 3)\n",
        "('-s0_caps_c_lots', 2)\n",
        "('-s0_caps_c_1-s1_caps_c_2', 2)\n",
        "('-s0_caps_c_3-s1_caps_c_lots', 2)\n",
        "('-s0_caps_c_lots-s1_caps_c_3', 2)\n",
        "('-s0_caps_c_lots-s1_caps_c_lots', 1)\n",
        "('-s0_caps_c_3', 1)\n",
        "('-s0_caps_c_lots-s1_caps_c_1', 1)\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts = defaultdict(int)\n",
      "\n",
      "for doc in train_apws:\n",
      "    model = RushModel(doc, history=2, features=set(['quotes']))\n",
      "    gold_trans = model.gold_transitions()\n",
      "    for trans in gold_trans:\n",
      "        for f in model.feature_map(trans):\n",
      "            counts[f] += 1\n",
      "\n",
      "fcounts = [(f, counts[f]) for f in counts]\n",
      "fcounts.sort(reverse=True, key=lambda a:a[1])\n",
      "for c in fcounts:\n",
      "    print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('-0_False', 100)\n",
        "('-1_False', 97)\n",
        "('-0_True-1_False', 38)\n",
        "('-0_False-1_True', 35)\n",
        "('-0_True-1_True', 21)\n",
        "('-1_True', 3)\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "ALL FEATURES"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataX = []\n",
      "dataY = []\n",
      "\n",
      "for doc in train_apws:\n",
      "    model = RushModel(doc, history=2)\n",
      "    gold_trans = model.gold_transitions()\n",
      "    dataX.append(model)\n",
      "    dataY.append(gold_trans)\n",
      "print \"{} instances loaded.\".format(len(dataX))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 instances loaded.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"running perceptron\"\n",
      "ptron.fit(dataX,dataY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running perceptron\n",
        "iteration 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"loading test data\"\n",
      "testX = []\n",
      "\n",
      "for doc in test_apws:\n",
      "    model = RushModel(doc, history=2)\n",
      "    testX.append(model)\n",
      "        \n",
      "print len(testX)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading test data\n",
        "99\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Predicting...\"\n",
      "predY = ptron.predict(testX)\n",
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import getenv\n",
      "from os.path import join\n",
      "\n",
      "pred_fname = '/tmp/predicted_file'\n",
      "pred_file = open(pred_fname, 'w')\n",
      "\n",
      "gold_fname = '/tmp/gold_file'\n",
      "gold_file = open(gold_fname, 'w')\n",
      "\n",
      "ddir = getenv(\"DISCOURSEDIR\",\".\")\n",
      "eval_script = join(ddir, \"eval\", \"ordering-eval.py\")\n",
      "\n",
      "for i, model in enumerate(testX):\n",
      "    pred_order = predY[i]\n",
      "    pred_str = model.ordering2str(pred_order)\n",
      "    \n",
      "    pred_file.write(pred_str+\"\\n\\n\")\n",
      "    pred_file.flush()\n",
      "    \n",
      "    gold_str = model.gold_str()\n",
      "    gold_file.write(gold_str+\"\\n\\n\")\n",
      "    gold_file.flush()\n",
      "\n",
      "pred_file.close()\n",
      "gold_file.close()\n",
      "\n",
      "!python $eval_script --gold $gold_fname --predicted $pred_fname     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}